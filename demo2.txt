Reading fron file
+---------+------+---------+-----------+
| driverId|  name|  address|       city|
+---------+------+---------+-----------+
|driver001|Ramesh|Hyderabad|  Hyderabad|
|driver002|Rakesh|Hyderabad|Hyderabad12|
+---------+------+---------+-----------+

Input Data schema
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

Input Data frame schema
root
 |-- key: string (nullable = true)
 |-- value: string (nullable = true)

schema
root
 |-- vechicleid: string (nullable = true)
 |-- driverId: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- latitude: double (nullable = true)
 |-- longitude: double (nullable = true)
 |-- speed: double (nullable = true)

join schema
root
 |-- driverId: string (nullable = true)
 |-- vechicleid: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- latitude: double (nullable = true)
 |-- longitude: double (nullable = true)
 |-- speed: double (nullable = true)
 |-- name: string (nullable = true)
 |-- address: string (nullable = true)
 |-- city: string (nullable = true)

+-----+---+--------+----+----------+------------------------+--------+---------+
|start|end|driverId|name|vechicleid|NumberofTimesOverSpeeded|latitude|longitude|
+-----+---+--------+----+----------+------------------------+--------+---------+
+-----+---+--------+----+----------+------------------------+--------+---------+

+-------------------+-------------------+---------+------+----------+------------------------+--------+---------+
|              start|                end| driverId|  name|vechicleid|NumberofTimesOverSpeeded|latitude|longitude|
+-------------------+-------------------+---------+------+----------+------------------------+--------+---------+
|2021-09-12 09:52:00|2021-09-12 09:54:00|driver002|Rakesh|vehicle002|                       1|      33|       96|
|2021-09-11 13:32:00|2021-09-11 13:34:00|driver001|Ramesh|vehicle001|                       1|      51|       -0|
|2021-09-06 16:06:00|2021-09-06 16:08:00|driver001|Ramesh|vehicle001|                       1|      51|       -0|
|2021-09-08 16:54:00|2021-09-08 16:56:00|driver001|Ramesh|vehicle001|                       4|      51|       -0|
|2021-09-11 15:36:00|2021-09-11 15:38:00|driver002|Rakesh|vehicle002|                       1|      33|       -9|
|2021-09-11 15:52:00|2021-09-11 15:54:00|driver001|Ramesh|vehicle001|                       1|      51|       -0|
|2021-09-11 15:06:00|2021-09-11 15:08:00|driver001|Ramesh|vehicle001|                       2|      51|       -0|
|2021-09-11 13:44:00|2021-09-11 13:46:00|driver001|Ramesh|vehicle001|                       2|      51|       -0|
|2021-09-11 15:12:00|2021-09-11 15:14:00|driver001|Ramesh|vehicle001|                       2|      51|       -0|
|2021-09-06 16:52:00|2021-09-06 16:54:00|driver001|Ramesh|vehicle001|                       2|      51|       -0|
|2021-09-11 16:22:00|2021-09-11 16:24:00|driver001|Ramesh|vehicle001|                       1|      51|       -0|
|2021-09-11 16:12:00|2021-09-11 16:14:00|driver001|Ramesh|vehicle001|                       2|      51|       -0|
|2021-09-11 16:56:00|2021-09-11 16:58:00|driver001|Ramesh|vehicle001|                       2|      51|       -0|
|2021-09-11 13:26:00|2021-09-11 13:28:00|driver001|Ramesh|vehicle001|                       1|      51|       -0|
|2021-09-11 13:00:00|2021-09-11 13:02:00|driver001|Ramesh|vehicle001|                       2|      51|       -0|
|2021-09-11 16:36:00|2021-09-11 16:38:00|driver001|Ramesh|vehicle001|                       1|      51|       -0|
|2021-09-06 17:22:00|2021-09-06 17:24:00|driver001|Ramesh|vehicle001|                       2|      51|       -0|
|2021-09-11 13:10:00|2021-09-11 13:12:00|driver001|Ramesh|vehicle001|                       1|      51|       -0|
|2021-09-11 13:52:00|2021-09-11 13:54:00|driver001|Ramesh|vehicle001|                       2|      33|       -9|
|2021-09-11 13:06:00|2021-09-11 13:08:00|driver001|Ramesh|vehicle001|                       2|      51|       -0|
+-------------------+-------------------+---------+------+----------+------------------------+--------+---------+
only showing top 20 rows

+-----+---+--------+----+----------+------------------------+--------+---------+
|start|end|driverId|name|vechicleid|NumberofTimesOverSpeeded|latitude|longitude|
+-----+---+--------+----+----------+------------------------+--------+---------+
+-----+---+--------+----+----------+------------------------+--------+---------+

+-----+---+--------+----+----------+------------------------+--------+---------+
|start|end|driverId|name|vechicleid|NumberofTimesOverSpeeded|latitude|longitude|
+-----+---+--------+----+----------+------------------------+--------+---------+
+-----+---+--------+----+----------+------------------------+--------+---------+

+-------------------+-------------------+---------+------+----------+------------------------+--------+---------+
|              start|                end| driverId|  name|vechicleid|NumberofTimesOverSpeeded|latitude|longitude|
+-------------------+-------------------+---------+------+----------+------------------------+--------+---------+
|2021-09-12 10:44:00|2021-09-12 10:46:00|driver002|Rakesh|vehicle002|                       1|      34|       97|
|2021-09-12 10:44:00|2021-09-12 10:46:00|driver001|Ramesh|vehicle001|                       1|      35|       98|
|2021-09-12 10:44:00|2021-09-12 10:46:00|driver001|Ramesh|vehicle001|                       1|      36|       99|
|2021-09-12 10:44:00|2021-09-12 10:46:00|driver002|Rakesh|vehicle002|                       1|      35|       98|
+-------------------+-------------------+---------+------+----------+------------------------+--------+---------+

+-----+---+--------+----+----------+------------------------+--------+---------+
|start|end|driverId|name|vechicleid|NumberofTimesOverSpeeded|latitude|longitude|
+-----+---+--------+----+----------+------------------------+--------+---------+
+-----+---+--------+----+----------+------------------------+--------+---------+

+-------------------+-------------------+---------+------+----------+------------------------+--------+---------+
|              start|                end| driverId|  name|vechicleid|NumberofTimesOverSpeeded|latitude|longitude|
+-------------------+-------------------+---------+------+----------+------------------------+--------+---------+
|2021-09-12 10:46:00|2021-09-12 10:48:00|driver002|Rakesh|vehicle002|                       2|      36|       99|
|2021-09-12 10:46:00|2021-09-12 10:48:00|driver001|Ramesh|vehicle001|                       2|      37|       99|
|2021-09-12 10:46:00|2021-09-12 10:48:00|driver002|Rakesh|vehicle002|                       1|      35|       98|
|2021-09-12 10:46:00|2021-09-12 10:48:00|driver001|Ramesh|vehicle001|                       1|      36|       99|
+-------------------+-------------------+---------+------+----------+------------------------+--------+---------+

+-----+---+--------+----+----------+------------------------+--------+---------+
|start|end|driverId|name|vechicleid|NumberofTimesOverSpeeded|latitude|longitude|
+-----+---+--------+----+----------+------------------------+--------+---------+
+-----+---+--------+----+----------+------------------------+--------+---------+

+-----+---+--------+----+----------+------------------------+--------+---------+
|start|end|driverId|name|vechicleid|NumberofTimesOverSpeeded|latitude|longitude|
+-----+---+--------+----+----------+------------------------+--------+---------+
+-----+---+--------+----+----------+------------------------+--------+---------+

+-------------------+-------------------+---------+------+----------+------------------------+--------+---------+
|              start|                end| driverId|  name|vechicleid|NumberofTimesOverSpeeded|latitude|longitude|
+-------------------+-------------------+---------+------+----------+------------------------+--------+---------+
|2021-09-12 10:48:00|2021-09-12 10:50:00|driver002|Rakesh|vehicle002|                       1|      37|       99|
|2021-09-12 10:48:00|2021-09-12 10:50:00|driver002|Rakesh|vehicle002|                       1|      33|       96|
|2021-09-12 10:48:00|2021-09-12 10:50:00|driver001|Ramesh|vehicle001|                       2|      33|       96|
+-------------------+-------------------+---------+------+----------+------------------------+--------+---------+

+-----+---+--------+----+----------+------------------------+--------+---------+
|start|end|driverId|name|vechicleid|NumberofTimesOverSpeeded|latitude|longitude|
+-----+---+--------+----+----------+------------------------+--------+---------+
+-----+---+--------+----+----------+------------------------+--------+---------+

+-----+---+--------+----+----------+------------------------+--------+---------+
|start|end|driverId|name|vechicleid|NumberofTimesOverSpeeded|latitude|longitude|
+-----+---+--------+----+----------+------------------------+--------+---------+
+-----+---+--------+----+----------+------------------------+--------+---------+

Traceback (most recent call last):
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 63, in deco
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o108.awaitTermination.
: org.apache.spark.sql.streaming.StreamingQueryException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 2381, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 191, in call
    raise e
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 188, in call
    self.func(DataFrame(jdf, self.sql_ctx), batch_id)
  File "C:/Users/Jagadeesh Panthati/Pictures/SPA_assignment/Solution1/spark_kafka.py", line 27, in writeToSinks
    .option("checkpointLocation", "checkpoints") \
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 740, in save
    self._jwrite.save()
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 63, in deco
    return f(*a, **kw)
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o249.save.
: org.apache.spark.SparkException: Job 84 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:954)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:952)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:952)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2164)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2077)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:980)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:978)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:978)
	at org.apache.spark.sql.kafka010.KafkaWriter$.write(KafkaWriter.scala:87)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:254)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:136)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:160)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:157)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)
	at sun.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)


=== Streaming Query ===
Identifier: [id = 503b6932-5af2-48f6-ac45-e0dd0391c519, runId = 803ce25c-3a6c-450e-bdf1-f896f0e150a7]
Current Committed Offsets: {KafkaV2[Subscribe[test1]]: {"test1":{"0":5600}}}
Current Available Offsets: {KafkaV2[Subscribe[test1]]: {"test1":{"0":5617}}}

Current State: ACTIVE
Thread State: RUNNABLE

Logical Plan:
Project [window#95-T120000ms.start AS start#116, window#95-T120000ms.end AS end#117, driverId#55, name#1, vechicleid#54, NumberofTimesOverSpeeded#107L, latitude#93, longitude#94]
+- Aggregate [name#1, driverId#55, vechicleid#54, substring(cast(latitude#57 as string), 1, 2), substring(cast(longitude#58 as string), 1, 2), window#108-T120000ms], [name#1, driverId#55, vechicleid#54, substring(cast(latitude#57 as string), 1, 2) AS latitude#93, substring(cast(longitude#58 as string), 1, 2) AS longitude#94, window#108-T120000ms AS window#95-T120000ms, count(1) AS NumberofTimesOverSpeeded#107L]
   +- Filter isnotnull(timestamp#56-T120000ms)
      +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 120000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 120000000) + 0) + 120000000), LongType, TimestampType)) AS window#108-T120000ms, driverId#55, vechicleid#54, timestamp#56-T120000ms, latitude#57, longitude#58, speed#59, name#1, address#2, city#3]
         +- EventTimeWatermark timestamp#56: timestamp, interval 2 minutes
            +- Filter (speed#59 > cast(50 as double))
               +- Project [driverId#55, vechicleid#54, timestamp#56, latitude#57, longitude#58, speed#59, name#1, address#2, city#3]
                  +- Project [driverId#55, vechicleid#54, timestamp#56, latitude#57, longitude#58, speed#59, name#1, address#2, city#3]
                     +- Join Inner, (driverId#55 = driverId#0)
                        :- Project [jsondata#50.vechicleid AS vechicleid#54, jsondata#50.driverId AS driverId#55, jsondata#50.timestamp AS timestamp#56, jsondata#50.latitude AS latitude#57, jsondata#50.longitude AS longitude#58, jsondata#50.speed AS speed#59]
                        :  +- Project [key#46, value#47, jsontostructs(StructField(vechicleid,StringType,true), StructField(driverId,StringType,true), StructField(timestamp,TimestampType,true), StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(speed,DoubleType,true), value#47, Some(Asia/Calcutta)) AS jsonData#50]
                        :     +- Project [cast(key#32 as string) AS key#46, cast(value#33 as string) AS value#47]
                        :        +- StreamingExecutionRelation KafkaV2[Subscribe[test1]], [key#32, value#33, topic#34, partition#35, offset#36L, timestamp#37, timestampType#38]
                        +- Relation[driverId#0,name#1,address#2,city#3] csv

	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:297)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:193)
Caused by: py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 2381, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 191, in call
    raise e
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 188, in call
    self.func(DataFrame(jdf, self.sql_ctx), batch_id)
  File "C:/Users/Jagadeesh Panthati/Pictures/SPA_assignment/Solution1/spark_kafka.py", line 27, in writeToSinks
    .option("checkpointLocation", "checkpoints") \
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 740, in save
    self._jwrite.save()
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 63, in deco
    return f(*a, **kw)
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o249.save.
: org.apache.spark.SparkException: Job 84 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:954)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:952)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:952)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2164)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2077)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:980)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:978)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:978)
	at org.apache.spark.sql.kafka010.KafkaWriter$.write(KafkaWriter.scala:87)
	at org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:254)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:136)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:160)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:157)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)
	at sun.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)


	at py4j.Protocol.getReturnValue(Protocol.java:473)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)
	at com.sun.proxy.$Proxy21.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$$anonfun$callForeachBatch$1.apply(ForeachBatchSink.scala:55)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$$anonfun$callForeachBatch$1.apply(ForeachBatchSink.scala:55)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5$$anonfun$apply$19.apply(MicroBatchExecution.scala:548)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5.apply(MicroBatchExecution.scala:546)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:545)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:198)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:160)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:281)
	... 1 more


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/Jagadeesh Panthati/Pictures/SPA_assignment/Solution1/spark_kafka.py", line 121, in <module>
    .outputMode("append")\
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\streaming.py", line 103, in awaitTermination
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 1257, in __call__
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 75, in deco
pyspark.sql.utils.StreamingQueryException: 'An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py", line 2381, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py", line 191, in call\n    raise e\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py", line 188, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File "C:/Users/Jagadeesh Panthati/Pictures/SPA_assignment/Solution1/spark_kafka.py", line 27, in writeToSinks\n    .option("checkpointLocation", "checkpoints") \\\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\readwriter.py", line 740, in save\n    self._jwrite.save()\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py", line 63, in deco\n    return f(*a, **kw)\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o249.save.\n: org.apache.spark.SparkException: Job 84 cancelled because SparkContext was shut down\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:954)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:952)\r\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\r\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:952)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2164)\r\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\r\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2077)\r\n\tat org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)\r\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)\r\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1948)\r\n\tat org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)\r\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:980)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:978)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:978)\r\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.write(KafkaWriter.scala:87)\r\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.createRelation(KafkaSourceProvider.scala:254)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:136)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:132)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:160)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:157)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\r\n\tat sun.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\n\n=== Streaming Query ===\nIdentifier: [id = 503b6932-5af2-48f6-ac45-e0dd0391c519, runId = 803ce25c-3a6c-450e-bdf1-f896f0e150a7]\nCurrent Committed Offsets: {KafkaV2[Subscribe[test1]]: {"test1":{"0":5600}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[test1]]: {"test1":{"0":5617}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nProject [window#95-T120000ms.start AS start#116, window#95-T120000ms.end AS end#117, driverId#55, name#1, vechicleid#54, NumberofTimesOverSpeeded#107L, latitude#93, longitude#94]\n+- Aggregate [name#1, driverId#55, vechicleid#54, substring(cast(latitude#57 as string), 1, 2), substring(cast(longitude#58 as string), 1, 2), window#108-T120000ms], [name#1, driverId#55, vechicleid#54, substring(cast(latitude#57 as string), 1, 2) AS latitude#93, substring(cast(longitude#58 as string), 1, 2) AS longitude#94, window#108-T120000ms AS window#95-T120000ms, count(1) AS NumberofTimesOverSpeeded#107L]\n   +- Filter isnotnull(timestamp#56-T120000ms)\n      +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 120000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#56-T120000ms, TimestampType, LongType) - 0) as double) / cast(120000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 120000000) + 0) + 120000000), LongType, TimestampType)) AS window#108-T120000ms, driverId#55, vechicleid#54, timestamp#56-T120000ms, latitude#57, longitude#58, speed#59, name#1, address#2, city#3]\n         +- EventTimeWatermark timestamp#56: timestamp, interval 2 minutes\n            +- Filter (speed#59 > cast(50 as double))\n               +- Project [driverId#55, vechicleid#54, timestamp#56, latitude#57, longitude#58, speed#59, name#1, address#2, city#3]\n                  +- Project [driverId#55, vechicleid#54, timestamp#56, latitude#57, longitude#58, speed#59, name#1, address#2, city#3]\n                     +- Join Inner, (driverId#55 = driverId#0)\n                        :- Project [jsondata#50.vechicleid AS vechicleid#54, jsondata#50.driverId AS driverId#55, jsondata#50.timestamp AS timestamp#56, jsondata#50.latitude AS latitude#57, jsondata#50.longitude AS longitude#58, jsondata#50.speed AS speed#59]\n                        :  +- Project [key#46, value#47, jsontostructs(StructField(vechicleid,StringType,true), StructField(driverId,StringType,true), StructField(timestamp,TimestampType,true), StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(speed,DoubleType,true), value#47, Some(Asia/Calcutta)) AS jsonData#50]\n                        :     +- Project [cast(key#32 as string) AS key#46, cast(value#33 as string) AS value#47]\n                        :        +- StreamingExecutionRelation KafkaV2[Subscribe[test1]], [key#32, value#33, topic#34, partition#35, offset#36L, timestamp#37, timestampType#38]\n                        +- Relation[driverId#0,name#1,address#2,city#3] csv\n'
Terminate batch job (Y/N)? 