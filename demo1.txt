Reading fron file
+---------+------+---------+-----------+
| driverId|  name|  address|       city|
+---------+------+---------+-----------+
|driver001|Ramesh|Hyderabad|  Hyderabad|
|driver002|Rakesh|Hyderabad|Hyderabad12|
+---------+------+---------+-----------+

Input Data schema
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

Input Data frame schema
root
 |-- key: string (nullable = true)
 |-- value: string (nullable = true)

schema
root
 |-- vechicleid: string (nullable = true)
 |-- driverId: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- latitude: double (nullable = true)
 |-- longitude: double (nullable = true)
 |-- speed: double (nullable = true)

join schema
root
 |-- driverId: string (nullable = true)
 |-- vechicleid: string (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- latitude: double (nullable = true)
 |-- longitude: double (nullable = true)
 |-- speed: double (nullable = true)
 |-- name: string (nullable = true)
 |-- address: string (nullable = true)
 |-- city: string (nullable = true)

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+---------+----------+--------------------+------------------+--------------------+-----+------+---------+---------+
| driverId|vechicleid|           timestamp|          latitude|           longitude|speed|  name|  address|     city|
+---------+----------+--------------------+------------------+--------------------+-----+------+---------+---------+
|driver001|vehicle001|2021-09-06 15:07:...|51.514030792232774|-0.10999202728271484| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 15:09:...| 51.50502957514356|-0.11312484741210938| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 15:10:...| 51.49901887040353| -0.1048421859741211| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 15:11:...|51.503667218218546|-0.10445594787597656| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 15:13:...|51.514030792232774|-0.10999202728271484| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 15:14:...| 51.50502957514356|-0.11312484741210938| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:05:...| 51.49901887040353| -0.1048421859741211| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:06:...|51.503667218218546|-0.10445594787597656| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:08:...|51.514030792232774|-0.10999202728271484| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:09:...| 51.50502957514356|-0.11312484741210938| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:11:...| 51.49901887040353| -0.1048421859741211| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:11:...|51.503667218218546|-0.10445594787597656| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:13:...|51.514030792232774|-0.10999202728271484| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:15:...| 51.50502957514356|-0.11312484741210938| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:16:...| 51.49901887040353| -0.1048421859741211| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:17:...|51.503667218218546|-0.10445594787597656| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:19:...|51.514030792232774|-0.10999202728271484| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:20:...| 51.50502957514356|-0.11312484741210938| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:22:...| 51.49901887040353| -0.1048421859741211| 60.0|Ramesh|Hyderabad|Hyderabad|
|driver001|vehicle001|2021-09-06 16:22:...|51.503667218218546|-0.10445594787597656| 60.0|Ramesh|Hyderabad|Hyderabad|
+---------+----------+--------------------+------------------+--------------------+-----+------+---------+---------+
only showing top 20 rows

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+---------+----------+--------------------+------------------+-----------------+-----+------+---------+---------+
| driverId|vechicleid|           timestamp|          latitude|        longitude|speed|  name|  address|     city|
+---------+----------+--------------------+------------------+-----------------+-----+------+---------+---------+
|driver001|vehicle001|2021-09-12 10:43:...|35.187217363783695|98.00448115620483| 60.0|Ramesh|Hyderabad|Hyderabad|
+---------+----------+--------------------+------------------+-----------------+-----+------+---------+---------+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+---------+----------+--------------------+-----------------+-----------------+-----+------+---------+-----------+
| driverId|vechicleid|           timestamp|         latitude|        longitude|speed|  name|  address|       city|
+---------+----------+--------------------+-----------------+-----------------+-----+------+---------+-----------+
|driver002|vehicle002|2021-09-12 10:44:...|34.05470560628565|97.63581421594682| 70.0|Rakesh|Hyderabad|Hyderabad12|
+---------+----------+--------------------+-----------------+-----------------+-----+------+---------+-----------+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+---------+----------+--------------------+------------------+-----------------+-----+------+---------+---------+
| driverId|vechicleid|           timestamp|          latitude|        longitude|speed|  name|  address|     city|
+---------+----------+--------------------+------------------+-----------------+-----+------+---------+---------+
|driver001|vehicle001|2021-09-12 10:44:...|35.305195153642465|98.32490108663032| 60.0|Ramesh|Hyderabad|Hyderabad|
+---------+----------+--------------------+------------------+-----------------+-----+------+---------+---------+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
+--------+----------+---------+--------+---------+-----+----+-------+----+
|driverId|vechicleid|timestamp|latitude|longitude|speed|name|address|city|
+--------+----------+---------+--------+---------+-----+----+-------+----+
+--------+----------+---------+--------+---------+-----+----+-------+----+

kafka_brokers:::localhost:9092
kafka_src_topic_name:::test1
kafka_dest_topic_name1:::dest1
Traceback (most recent call last):
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 63, in deco
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o77.awaitTermination.
: org.apache.spark.sql.streaming.StreamingQueryException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 2381, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 191, in call
    raise e
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 188, in call
    self.func(DataFrame(jdf, self.sql_ctx), batch_id)
  File "C:/Users/Jagadeesh Panthati/Pictures/SPA_assignment/Solution1/spark_kafka_q6.py", line 29, in writeToSinks
    stream_df.write.format("console").save()
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 740, in save
    self._jwrite.save()
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 63, in deco
    return f(*a, **kw)
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o440.save.
: org.apache.spark.SparkException: Job 99 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:954)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:952)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:952)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2164)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2077)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:370)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3388)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:751)
	at org.apache.spark.sql.execution.streaming.ConsoleSinkProvider.createRelation(console.scala:56)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:136)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:160)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:157)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)
	at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)


=== Streaming Query ===
Identifier: [id = 5dce1db6-1ffc-4431-b8a0-57e4b5f58bdd, runId = d3233f31-edd0-4f21-bdff-97bde8bbbb0c]
Current Committed Offsets: {KafkaV2[Subscribe[test1]]: {"test1":{"0":5375}}}
Current Available Offsets: {KafkaV2[Subscribe[test1]]: {"test1":{"0":5376}}}

Current State: ACTIVE
Thread State: RUNNABLE

Logical Plan:
Filter (speed#59 > cast(50 as double))
+- Project [driverId#55, vechicleid#54, timestamp#56, latitude#57, longitude#58, speed#59, name#1, address#2, city#3]
   +- Project [driverId#55, vechicleid#54, timestamp#56, latitude#57, longitude#58, speed#59, name#1, address#2, city#3]
      +- Join Inner, (driverId#55 = driverId#0)
         :- Project [jsondata#50.vechicleid AS vechicleid#54, jsondata#50.driverId AS driverId#55, jsondata#50.timestamp AS timestamp#56, jsondata#50.latitude AS latitude#57, jsondata#50.longitude AS longitude#58, jsondata#50.speed AS speed#59]
         :  +- Project [key#46, value#47, jsontostructs(StructField(vechicleid,StringType,true), StructField(driverId,StringType,true), StructField(timestamp,TimestampType,true), StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(speed,DoubleType,true), value#47, Some(Asia/Calcutta)) AS jsonData#50]
         :     +- Project [cast(key#32 as string) AS key#46, cast(value#33 as string) AS value#47]
         :        +- StreamingExecutionRelation KafkaV2[Subscribe[test1]], [key#32, value#33, topic#34, partition#35, offset#36L, timestamp#37, timestampType#38]
         +- Relation[driverId#0,name#1,address#2,city#3] csv

	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:297)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:193)
Caused by: py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 2381, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 191, in call
    raise e
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 188, in call
    self.func(DataFrame(jdf, self.sql_ctx), batch_id)
  File "C:/Users/Jagadeesh Panthati/Pictures/SPA_assignment/Solution1/spark_kafka_q6.py", line 29, in writeToSinks
    stream_df.write.format("console").save()
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 740, in save
    self._jwrite.save()
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 63, in deco
    return f(*a, **kw)
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o440.save.
: org.apache.spark.SparkException: Job 99 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:954)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:952)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:952)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2164)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2077)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:370)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3388)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:751)
	at org.apache.spark.sql.execution.streaming.ConsoleSinkProvider.createRelation(console.scala:56)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:136)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:160)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:157)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)
	at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)


	at py4j.Protocol.getReturnValue(Protocol.java:473)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)
	at com.sun.proxy.$Proxy20.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$$anonfun$callForeachBatch$1.apply(ForeachBatchSink.scala:55)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$$anonfun$callForeachBatch$1.apply(ForeachBatchSink.scala:55)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5$$anonfun$apply$19.apply(MicroBatchExecution.scala:548)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5.apply(MicroBatchExecution.scala:546)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:545)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:198)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:166)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:160)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:281)
	... 1 more


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/Jagadeesh Panthati/Pictures/SPA_assignment/Solution1/spark_kafka_q6.py", line 130, in <module>
    .outputMode("append") \
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\streaming.py", line 103, in awaitTermination
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 1257, in __call__
  File "C:\spark-2.4.8-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 75, in deco
pyspark.sql.utils.StreamingQueryException: 'An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py", line 2381, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py", line 191, in call\n    raise e\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py", line 188, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File "C:/Users/Jagadeesh Panthati/Pictures/SPA_assignment/Solution1/spark_kafka_q6.py", line 29, in writeToSinks\n    stream_df.write.format("console").save()\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\readwriter.py", line 740, in save\n    self._jwrite.save()\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py", line 63, in deco\n    return f(*a, **kw)\n  File "C:\\spark-2.4.8-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o440.save.\n: org.apache.spark.SparkException: Job 99 cancelled because SparkContext was shut down\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:954)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:952)\r\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\r\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:952)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2164)\r\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\r\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2077)\r\n\tat org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)\r\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)\r\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1948)\r\n\tat org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)\r\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\r\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2107)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:370)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3388)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3368)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\r\n\tat org.apache.spark.sql.Dataset.show(Dataset.scala:751)\r\n\tat org.apache.spark.sql.execution.streaming.ConsoleSinkProvider.createRelation(console.scala:56)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:136)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:132)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:160)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:157)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\r\n\tat sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\n\n=== Streaming Query ===\nIdentifier: [id = 5dce1db6-1ffc-4431-b8a0-57e4b5f58bdd, runId = d3233f31-edd0-4f21-bdff-97bde8bbbb0c]\nCurrent Committed Offsets: {KafkaV2[Subscribe[test1]]: {"test1":{"0":5375}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[test1]]: {"test1":{"0":5376}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nFilter (speed#59 > cast(50 as double))\n+- Project [driverId#55, vechicleid#54, timestamp#56, latitude#57, longitude#58, speed#59, name#1, address#2, city#3]\n   +- Project [driverId#55, vechicleid#54, timestamp#56, latitude#57, longitude#58, speed#59, name#1, address#2, city#3]\n      +- Join Inner, (driverId#55 = driverId#0)\n         :- Project [jsondata#50.vechicleid AS vechicleid#54, jsondata#50.driverId AS driverId#55, jsondata#50.timestamp AS timestamp#56, jsondata#50.latitude AS latitude#57, jsondata#50.longitude AS longitude#58, jsondata#50.speed AS speed#59]\n         :  +- Project [key#46, value#47, jsontostructs(StructField(vechicleid,StringType,true), StructField(driverId,StringType,true), StructField(timestamp,TimestampType,true), StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(speed,DoubleType,true), value#47, Some(Asia/Calcutta)) AS jsonData#50]\n         :     +- Project [cast(key#32 as string) AS key#46, cast(value#33 as string) AS value#47]\n         :        +- StreamingExecutionRelation KafkaV2[Subscribe[test1]], [key#32, value#33, topic#34, partition#35, offset#36L, timestamp#37, timestampType#38]\n         +- Relation[driverId#0,name#1,address#2,city#3] csv\n'
Terminate batch job (Y/N)? 
Terminate batch job (Y/N)? Terminate batch job (Y/N)? 
